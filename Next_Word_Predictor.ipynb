{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPjCqL23HrM64e4YKxqcaU7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dynamic369/Next_Word_Prediction/blob/main/Next_Word_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load the data and save it in any text file\n",
        "2. data preprocessing\n",
        "\n",
        "  :-open the data from that text file\n",
        "\n",
        "  :- tokenize the text\n",
        "\n",
        "  :- create the input sequence so that model is able to learn to predict the next words\n",
        "\n",
        "  :- paddding (Padding is necessary because neural networks require inputs of the same size,)\n",
        "  \n",
        "  :- text split into x,y and then train and text data\n",
        "\n",
        "3. Defining my Sequential LSTM model that contain one embedding, two LSTM , one Dropout and one Dense layer.\n",
        "4. Train the model.\n",
        "5. Make a prediction function for prediction."
      ],
      "metadata": {
        "id": "wC6F2xbgGiBQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IZHKyNfJE2W"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download(\"gutenberg\")\n",
        "from nltk.corpus import gutenberg\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = gutenberg.raw('shakespeare-hamlet.txt')\n",
        "# save the file\n",
        "with open('hamet.txt','w') as file:\n",
        "  file.write(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of words\n",
        "len(data)"
      ],
      "metadata": {
        "id": "9ruM-djNKDa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# open the dataset\n",
        "with open('/content/hamet.txt','r') as file:\n",
        "  text = file.read()\n",
        "\n",
        "#tokenize the text-creating indexes for words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index)+1\n",
        "total_words\n"
      ],
      "metadata": {
        "id": "nIwy9lTwKDX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "id": "1yD7HI1oKDVN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create my input sequence\n",
        "input_sequences = []\n",
        "for line in text.split(\"\\n\"):\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "  for i in range(1,len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "qKz3e1cqKDSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "id": "b_mo1ZdAKDPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_len = max([len(x) for x in input_sequences])\n"
      ],
      "metadata": {
        "id": "I60cTkhlKDM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = np.array(pad_sequences(input_sequences,maxlen=max_sequence_len,padding='pre'))\n",
        "input_sequences"
      ],
      "metadata": {
        "id": "q6-JZEj3EOem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 create predictors and label\n",
        "import tensorflow as tf\n",
        "x,y = input_sequences[:,:-1],input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "SvuAcI5_Fqw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = tf.keras.utils.to_categorical(y,num_classes=total_words)"
      ],
      "metadata": {
        "id": "N_DQtAFxF7gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "w7CiuoBGGLmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into train test\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "N2lB46JOGMEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stop = EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)"
      ],
      "metadata": {
        "id": "GQZw9cxQPyzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train our LSTM RNN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
        "\n",
        "## Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words,100,input_shape=(max_sequence_len-1,)))\n",
        "model.add(LSTM(150,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words,activation='softmax'))\n",
        "\n",
        "#compile the model\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ckzzK6BsIZr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(x_train,y_train,epochs=100,validation_data=(x_test,y_test),verbose=1)"
      ],
      "metadata": {
        "id": "r74FCFIHIZm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict the next word.\n",
        "def predict_next_word(model,tokenizer, text, max_sequence_len):\n",
        "  token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "  if len(token_list) >= max_sequence_len:\n",
        "    token_list = token_list[-(max_sequence_len-1):]\n",
        "\n",
        "  token_list = pad_sequences([token_list],maxlen=max_sequence_len-1,padding='pre')\n",
        "  predicted = model.predict(token_list,verbose=0)\n",
        "  predicted_word_index = np.argmax(predicted,axis=1)\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted_word_index:\n",
        "      return word\n",
        "  return None\n"
      ],
      "metadata": {
        "id": "CuJr5Q8fYGpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"When yond same Starre that's Westward from the\"\n",
        "print(f\"Input text: {input_text}\")\n",
        "max_sequence_len = model.input_shape[1]+1\n",
        "next_word = predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
        "print(f\"Next word: {next_word}\")"
      ],
      "metadata": {
        "id": "dmM1mC2RYGmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = input(\"Enter the text\")\n",
        "print(f\"Input text: {input_text}\")\n",
        "max_sequence_len = model.input_shape[1]+1\n",
        "next_word = predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
        "print(f\"Next word: {next_word}\")"
      ],
      "metadata": {
        "id": "1Oraw-ZvYGjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jsz80X0cdXLa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}